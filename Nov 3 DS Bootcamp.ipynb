{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name:** Prathamesh Adarkar\n",
    "**Email:** pa2529@nyu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca1215f-525a-4fd4-8653-842279e505da",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Split Ratios:**\n",
    "- Training: 70 speakers (70%)\n",
    "- Validation: 15 speakers (15%)\n",
    "- Test: 15 speakers (15%)\n",
    "\n",
    "### Critical Principle: NO SPEAKER OVERLAP\n",
    "\n",
    "Each speaker's entire 5-day recording sequence stays in ONE split only.\n",
    "\n",
    "### Reasoning:\n",
    "\n",
    "**1. Why Speaker-Level Splitting?**\n",
    "\n",
    "The goal is generalization to NEW speakers. If we split recordings randomly:\n",
    "- Same speaker appears in train AND test\n",
    "- Model learns speaker-specific patterns (voice characteristics, accent)\n",
    "- Test accuracy becomes artificially high\n",
    "- Model fails in deployment on unseen speakers\n",
    "\n",
    "**2. Why Keep All 5 Days Together?**\n",
    "\n",
    "Recordings from the same speaker across days are correlated:\n",
    "- Same microphone setup\n",
    "- Consistent background environment\n",
    "- Similar vocal characteristics\n",
    "\n",
    "Splitting days across sets causes data leakage.\n",
    "\n",
    "**3. Stratification Considerations:**\n",
    "\n",
    "Balance across splits:\n",
    "- Gender distribution (M/F)\n",
    "- Age groups\n",
    "- Accents/dialects\n",
    "- Ensure all 44 phones are well-represented\n",
    "\n",
    "**4. What We're Testing:**\n",
    "\n",
    "Test set evaluates: \"Can the model recognize phones from speakers it has NEVER seen?\"\n",
    "\n",
    "This matches real-world deployment where users are always new.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 2: Adding Kilian's 10,000 Recordings\n",
    "\n",
    "1. Maintain generalization to unseen speakers\n",
    "2. Optimize performance specifically for Kilian\n",
    "\n",
    "### Proposed Strategy: **Two-Stage Fine-Tuning**\n",
    "\n",
    "#### **Stage 1: Train Base Model**\n",
    "\n",
    "```\n",
    "Training:   70 speakers (original dataset)\n",
    "Validation: 15 speakers (original dataset)\n",
    "Test:       15 speakers (original dataset - held out)\n",
    "```\n",
    "\n",
    "**Output:** Base model θ_base with strong speaker-independent performance\n",
    "\n",
    "#### **Stage 2: Kilian-Specific Adaptation**\n",
    "\n",
    "**Split Kilian's 10,000 recordings:**\n",
    "```\n",
    "Training:   7,000 recordings (70%)\n",
    "Validation: 1,500 recordings (15%)  \n",
    "Test:       1,500 recordings (15%)\n",
    "```\n",
    "\n",
    "**Important:** Use temporal or random split for Kilian's data since it's all one speaker.\n",
    "\n",
    "**Fine-Tuning Protocol:**\n",
    "- Start from θ_base (pre-trained model)\n",
    "- Use MUCH smaller learning rate (10× smaller)\n",
    "- Apply regularization to prevent catastrophic forgetting\n",
    "- Monitor BOTH metrics during training:\n",
    "  - Performance on Kilian's validation set\n",
    "  - Performance on original 15-speaker test set\n",
    "\n",
    "**Stop Condition:**\n",
    "- If general test accuracy drops >2%, stop or reduce learning rate\n",
    "- Kilian validation accuracy should significantly improve\n",
    "\n",
    "#### **Final Evaluation:**\n",
    "\n",
    "Report TWO separate metrics:\n",
    "1. **General Performance:** Accuracy on 15 unseen speakers (should stay ≥98% of base)\n",
    "2. **Kilian Performance:** Accuracy on Kilian's test set (should improve significantly)\n",
    "\n",
    "### Why This Works:\n",
    "\n",
    "**Base Model Benefits:**\n",
    "- Learns speaker-invariant phone features from diverse data\n",
    "- Strong foundation for generalization\n",
    "\n",
    "**Fine-Tuning Benefits:**\n",
    "- Adapts to Kilian's specific voice characteristics\n",
    "- 10,000 samples is sufficient for meaningful adaptation\n",
    "- Low learning rate + regularization prevents forgetting general patterns\n",
    "\n",
    "### Alternative: Multi-Task Learning\n",
    "\n",
    "Instead of two stages, train ONE model with combined loss:\n",
    "\n",
    "```\n",
    "Loss = α × Loss_general + β × Loss_kilian\n",
    "\n",
    "Where α=0.7, β=0.3\n",
    "```\n",
    "\n",
    "**Data Split:**\n",
    "```\n",
    "Training:   70 general speakers + 5,000 Kilian recordings\n",
    "Validation: 15 general speakers + 1,500 Kilian recordings\n",
    "Test:       15 general speakers + 1,500 Kilian recordings (separate evaluation)\n",
    "```\n",
    "\n",
    "This maintains generalization while learning Kilian-specific patterns, but is more complex to tune.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Always split by speaker, never by recordings** (for generalization tasks)\n",
    "**Keep temporal sequences together** (prevent data leakage)\n",
    "\n",
    "**Fine-tuning requires regularization** (prevent catastrophic forgetting)\n",
    "\n",
    "**Dual evaluation is essential** (track both general and specific performance)\n",
    "\n",
    "**Never** randomly split recordings across train/val/test with same speakers\n",
    "\n",
    "**Never** fine-tune without monitoring general performance degradation\n",
    "\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- General test accuracy ≥ 98% of base model (≤2% forgetting)\n",
    "- Kilian test accuracy shows ≥10% improvement over base model\n",
    "- All 44 phones adequately represented in all splits\n",
    "- Balanced demographic distribution across splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors (K-NN) - Theory Solutions\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 1: 1-NN Classification & Decision Boundary\n",
    "\n",
    "### Dataset:\n",
    "- **Positive:** (1,2), (1,4), (5,4)\n",
    "- **Negative:** (3,1), (3,2)\n",
    "\n",
    "### 1-NN Decision Boundary Description:\n",
    "\n",
    "The 1-NN decision boundary is defined by the **perpendicular bisectors** between each positive-negative point pair. The boundary consists of piecewise linear segments (Voronoi diagram).\n",
    "\n",
    "**Key Boundary Regions:**\n",
    "\n",
    "```\n",
    "Visual Plot Description:\n",
    "\n",
    "      5 |\n",
    "      4 |    P₁ -------- P₃\n",
    "      3 |         |  \n",
    "      2 |    P₂   |   N₂      Decision boundary (approximate)\n",
    "      1 |         N₁ |\n",
    "      0 |____________|_________\n",
    "        0   1   2   3   4   5\n",
    "\n",
    "Where:\n",
    "- P₁ = (1,2) Positive\n",
    "- P₂ = (1,4) Positive  \n",
    "- P₃ = (5,4) Positive\n",
    "- N₁ = (3,1) Negative\n",
    "- N₂ = (3,2) Negative\n",
    "```\n",
    "\n",
    "**Decision Boundary Explanation:**\n",
    "\n",
    "The boundary forms perpendicular bisectors between nearest opposite-class neighbors:\n",
    "- Between P₁(1,2) and N₁(3,1): bisector at approximately x≈2, y≈1.5\n",
    "- Between P₁(1,2) and N₂(3,2): vertical bisector at x=2\n",
    "- Between P₂(1,4) and N₂(3,2): bisector at approximately x≈2, y≈3\n",
    "- Between P₃(5,4) and N₂(3,2): bisector separating right region\n",
    "\n",
    "**Example Classifications:**\n",
    "\n",
    "| Test Point | Nearest Neighbor | Distance | Classification |\n",
    "|------------|------------------|----------|----------------|\n",
    "| (2, 3)     | P₂ (1,4)        | √2 ≈ 1.4 | **Positive**   |\n",
    "| (3.5, 1.5) | N₁ (3,1)        | √0.5     | **Negative**   |\n",
    "| (4, 3)     | P₃ (5,4)        | √2 ≈ 1.4 | **Positive**   |\n",
    "| (2, 1)     | N₁ (3,1)        | 1.0      | **Negative**   |\n",
    "\n",
    "**Key Insight:** Any point is classified by its single nearest neighbor. The decision boundary creates regions (Voronoi cells) where all points in a region belong to the same class.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 2: Feature Scaling Impact\n",
    "\n",
    "### Original Dataset (Unscaled):\n",
    "- **Positive:** (100,2), (100,4), (500,4)\n",
    "- **Negative:** (300,1), (300,2)\n",
    "- **Test Point:** (500,1)\n",
    "\n",
    "### Before Scaling:\n",
    "\n",
    "**Distances from (500,1):**\n",
    "\n",
    "| Point       | Class    | Distance Calculation                        | Distance |\n",
    "|-------------|----------|---------------------------------------------|----------|\n",
    "| (100,2)     | Positive | √[(500-100)² + (1-2)²] = √[160000+1]      | 400.0    |\n",
    "| (100,4)     | Positive | √[(500-100)² + (1-4)²] = √[160000+9]      | 400.01   |\n",
    "| (500,4)     | Positive | √[(500-500)² + (1-4)²] = √[0+9]           | **3.0**  |\n",
    "| (300,1)     | Negative | √[(500-300)² + (1-1)²] = √[40000+0]       | 200.0    |\n",
    "| (300,2)     | Negative | √[(500-300)² + (1-2)²] = √[40000+1]       | 200.0    |\n",
    "\n",
    "**Nearest Neighbor:** (500,4) - Positive class  \n",
    "**Classification:**  **Positive**\n",
    "\n",
    "**Problem:** Feature 1 (range: 100-500) dominates Feature 2 (range: 1-4) due to scale difference!\n",
    "\n",
    "---\n",
    "\n",
    "### After Scaling to [0,1]:\n",
    "\n",
    "**Scaling Formula:** x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "**Feature 1:** min=100, max=500 → range=400  \n",
    "**Feature 2:** min=1, max=4 → range=3\n",
    "\n",
    "**Scaled Dataset:**\n",
    "\n",
    "| Original    | Feature 1 Scaled       | Feature 2 Scaled       | Scaled Point  |\n",
    "|-------------|------------------------|------------------------|---------------|\n",
    "| (100,2)     | (100-100)/400 = 0      | (2-1)/3 = 0.33         | (0, 0.33)     |\n",
    "| (100,4)     | 0                      | (4-1)/3 = 1.0          | (0, 1.0)      |\n",
    "| (500,4)     | (500-100)/400 = 1.0    | 1.0                    | (1.0, 1.0)    |\n",
    "| (300,1)     | (300-100)/400 = 0.5    | (1-1)/3 = 0            | (0.5, 0)      |\n",
    "| (300,2)     | 0.5                    | 0.33                   | (0.5, 0.33)   |\n",
    "| **(500,1)** | **1.0**                | **0**                  | **(1.0, 0)**  |\n",
    "\n",
    "**Distances from scaled (1.0, 0):**\n",
    "\n",
    "| Scaled Point | Class    | Distance Calculation                    | Distance |\n",
    "|--------------|----------|-----------------------------------------|----------|\n",
    "| (0, 0.33)    | Positive | √[(1-0)² + (0-0.33)²] = √[1+0.11]      | 1.05     |\n",
    "| (0, 1.0)     | Positive | √[(1-0)² + (0-1)²] = √[1+1]            | 1.41     |\n",
    "| (1.0, 1.0)   | Positive | √[(1-1)² + (0-1)²] = √[0+1]            | 1.0      |\n",
    "| (0.5, 0)     | Negative | √[(1-0.5)² + (0-0)²] = √[0.25+0]      | **0.5**  |\n",
    "| (0.5, 0.33)  | Negative | √[(1-0.5)² + (0-0.33)²] = √[0.25+0.11] | 0.6      |\n",
    "\n",
    "**Nearest Neighbor:** (0.5, 0) - Negative class  \n",
    "**Classification:**  **Negative**\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Condition      | Nearest Neighbor | Classification | Distance |\n",
    "|----------------|------------------|----------------|----------|\n",
    "| **Unscaled**   | (500,4)          | Positive       | 3.0      |\n",
    "| **Scaled**     | (300,1)          | Negative       | 0.5      |\n",
    "\n",
    "**Key Insight:**  \n",
    "Without scaling, Feature 1 (large scale) dominated, making horizontal distance the primary factor. After scaling, both features contribute equally, revealing that (500,1) is actually closer to the negative class in the normalized space.\n",
    "\n",
    "**Lesson:** Always scale features when they have different ranges, or K-NN will be biased toward high-magnitude features!\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 3: Handling Missing Values in K-NN\n",
    "\n",
    "### Challenge:\n",
    "Test point has missing features: e.g., (5, ?, 3) where feature 2 is missing.\n",
    "\n",
    "### Solution: Modified Distance Metric\n",
    "\n",
    "**Approach:** Only compute distance over available features and normalize.\n",
    "\n",
    "**Standard Euclidean Distance:**\n",
    "```\n",
    "d(x, y) = √[Σᵢ (xᵢ - yᵢ)²]\n",
    "```\n",
    "\n",
    "**Modified Distance (with missing values):**\n",
    "```\n",
    "d_modified(x, y) = √[n/n_available × Σᵢ∈available (xᵢ - yᵢ)²]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- n = total features\n",
    "- n_available = non-missing features\n",
    "- Scaling factor (n/n_available) normalizes for dimensionality\n",
    "\n",
    "**Example:**\n",
    "- Training: (2, 4, 3)\n",
    "- Test: (5, ?, 3) — Feature 2 missing\n",
    "- Distance: √[3/2 × ((5-2)² + (3-3)²)] = √[1.5 × 9] = 3.67\n",
    "\n",
    "**Why this works:** \n",
    "- Simple to implement\n",
    "- No data imputation needed\n",
    "- Uses only reliable information\n",
    "- Normalizes for reduced dimensionality\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 4: K-NN Success in High-Dimensional Image Data\n",
    "\n",
    "### Apparent Paradox:\n",
    "Images have thousands of pixels (dimensions), but K-NN still works well. Why doesn't the \"curse of dimensionality\" destroy performance?\n",
    "\n",
    "---\n",
    "\n",
    "### Reason 1: **Intrinsic Dimensionality is Low**\n",
    "\n",
    "**Explanation:**  \n",
    "Although images nominally have d=10,000+ dimensions (pixels), the **effective dimensionality is much lower**.\n",
    "\n",
    "**Why:**\n",
    "- Natural images lie on a **low-dimensional manifold** embedded in high-dimensional space\n",
    "- Pixels are highly correlated (e.g., neighboring pixels similar)\n",
    "- Images have structure: edges, textures, objects\n",
    "- Most of the 10,000 dimensions are redundant\n",
    "\n",
    "**Analogy:**  \n",
    "A 2D sheet of paper (low intrinsic dimension) can exist in 3D space. Images are like a low-D manifold in high-D pixel space.\n",
    "\n",
    "**Evidence:**\n",
    "- PCA on images: 95% variance captured by ~100 components (out of 10,000)\n",
    "- This means effective dimension ≈ 100, not 10,000\n",
    "\n",
    "---\n",
    "\n",
    "### Reason 2: **Distance Metrics Still Meaningful**\n",
    "\n",
    "**The Curse of Dimensionality predicts:**\n",
    "In high dimensions, all distances become similar → nearest neighbors aren't actually \"near.\"\n",
    "\n",
    "**Why this doesn't happen for images:**\n",
    "\n",
    "1. **Structured Data:** Image pixels aren't random; they have spatial correlation\n",
    "2. **Relevant Dimensions Dominate:** Discriminative features (edges, shapes) vary more than noise\n",
    "3. **L2 Distance Works:** Euclidean distance still captures perceptual similarity for images\n",
    "\n",
    "**Example:**\n",
    "- Two images of dogs: differ mainly in pose/lighting (few dimensions)\n",
    "- Dog vs. car: differ in fundamental structure (many dimensions)\n",
    "- Distance ratio: similar dogs are MUCH closer than dog-car\n",
    "\n",
    "---\n",
    "\n",
    "### Reason 3: **Large Training Sets**\n",
    "\n",
    "**Explanation:**  \n",
    "K-NN requires dense coverage of the feature space. Image datasets often have:\n",
    "- ImageNet: 1.2M images\n",
    "- MNIST: 60K images\n",
    "\n",
    "**Why this helps:**\n",
    "- Even in high-D space, with enough data, neighborhoods are well-populated\n",
    "- Rule of thumb: need N ≈ e^d samples for curse to dominate\n",
    "- But if intrinsic dimension is low (d_eff ≈ 100), N=60,000 is sufficient\n",
    "\n",
    "---\n",
    "\n",
    "### Reason 4: **Locality-Sensitive Hashing (LSH) and Approximate K-NN**\n",
    "\n",
    "**Modern K-NN systems use:**\n",
    "- Approximate nearest neighbor search (e.g., LSH, Annoy, FAISS)\n",
    "- These methods exploit low intrinsic dimensionality\n",
    "- Trade exact search for speed without significant accuracy loss\n",
    "\n",
    "---\n",
    "\n",
    "### Reason 5: **Feature Engineering / Learned Representations**\n",
    "\n",
    "**In practice, K-NN on images often uses:**\n",
    "- Pre-extracted features (HOG, SIFT)\n",
    "- Deep learning embeddings (CNN features)\n",
    "\n",
    "**Example:**\n",
    "- Raw pixels: 28×28 = 784 dimensions\n",
    "- CNN embedding: 128 dimensions\n",
    "- Effective dimension after CNN: ~20-50\n",
    "\n",
    "K-NN in embedding space works even better!\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Insight:\n",
    "\n",
    "**Curse of Dimensionality predicts:**\n",
    "```\n",
    "Distance concentration: max_dist / min_dist → 1 as d → ∞\n",
    "```\n",
    "\n",
    "**But for real images:**\n",
    "```\n",
    "max_dist / min_dist ≈ 2-5 (still distinguishable!)\n",
    "```\n",
    "\n",
    "Because intrinsic dimension d_eff << d_nominal.\n",
    "\n",
    "---\n",
    "\n",
    "### Empirical Evidence:\n",
    "\n",
    "**MNIST (28×28 = 784 dimensions):**\n",
    "- 1-NN achieves ~97% accuracy\n",
    "- If curse of dimensionality applied, would get ~50% (random)\n",
    "\n",
    "**CIFAR-10 (32×32×3 = 3,072 dimensions):**\n",
    "- K-NN with proper distance metric: ~35-40% accuracy\n",
    "- With learned features: ~80% accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Answer:\n",
    "\n",
    "**K-NN works well on high-dimensional images because:**\n",
    "\n",
    "1. **Low intrinsic dimensionality** — Images lie on low-D manifolds\n",
    "2.  **Structured correlations** — Pixels aren't independent random variables\n",
    "3. **Meaningful distances** — Perceptual similarity preserved by L2 distance\n",
    "4. **Large datasets** — Modern datasets provide dense coverage\n",
    "5. **Feature engineering** — CNNs/HOG reduce effective dimensionality further\n",
    "\n",
    "**Key Takeaway:**  \n",
    "The curse of dimensionality assumes uniformly distributed data in all dimensions. Real-world images violate this assumption spectacularly — they're highly structured, correlated, and low-dimensional in their essence.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes:\n",
    "\n",
    "**When K-NN FAILS on images:**\n",
    "- Very few training examples per class\n",
    "- High noise/occlusion\n",
    "- Need rotation/scale invariance (raw pixels aren't invariant)\n",
    "\n",
    "**When to use K-NN for images:**\n",
    "- Quick baseline\n",
    "- Small datasets where deep learning overfits\n",
    "- With good feature extraction (HOG, SIFT, CNN embeddings)\n",
    "- Non-parametric modeling needed\n",
    "\n",
    "**Best Practice:**\n",
    "- Always use feature extraction (don't use raw pixels)\n",
    "- Scale features to [0,1] or standardize\n",
    "- Use approximate K-NN (FAISS) for large datasets\n",
    "- Try different distance metrics (L1, L2, cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    " 1: NO - Just computing h(x) on training and test sets doesn't help determine if test error is meaningfully higher. \n",
    "You need statistical validation (confidence intervals, cross-validation, or hypothesis testing) to know if the\n",
    "difference is real or just random variation.\n",
    "\n",
    "\n",
    "2: Training error doesn't need explicit computation because the Perceptron convergence theorem \n",
    "guarantees zero training error when data is linearly separable. The algorithm only stops when\n",
    "all training points are correctly classified, so training error = 0% by construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration 1\n",
    "Starting with w₀ = (0, 0)\n",
    "Check x₁ = (10, -2), y₁ = +1:\n",
    "\n",
    "w₀ · x₁ = (0)(10) + (0)(-2) = 0\n",
    "y₁ · (w₀ · x₁) = (+1)(0) = 0 ≤ 0 ✗ Misclassified!\n",
    "\n",
    "Update:\n",
    "\n",
    "w₁ = w₀ + η · y₁ · x₁\n",
    "w₁ = (0, 0) + 1 · (+1) · (10, -2)\n",
    "w₁ = (10, -2)\n",
    "\n",
    "\n",
    "Iteration 2\n",
    "Check x₂ = (12, 2), y₂ = -1:\n",
    "\n",
    "w₁ · x₂ = (10)(12) + (-2)(2) = 120 - 4 = 116\n",
    "y₂ · (w₁ · x₂) = (-1)(116) = -116 ≤ 0 ✗ Misclassified!\n",
    "\n",
    "Update:\n",
    "\n",
    "w₂ = w₁ + η · y₂ · x₂\n",
    "w₂ = (10, -2) + 1 · (-1) · (12, 2)\n",
    "w₂ = (10, -2) + (-12, -2)\n",
    "w₂ = (-2, -4)\n",
    "\n",
    "\n",
    "Iteration 3\n",
    "Check x₁ = (10, -2), y₁ = +1:\n",
    "\n",
    "w₂ · x₁ = (-2)(10) + (-4)(-2) = -20 + 8 = -12\n",
    "y₁ · (w₂ · x₁) = (+1)(-12) = -12 ≤ 0 ✗ Misclassified!\n",
    "\n",
    "Update:\n",
    "\n",
    "w₃ = w₂ + η · y₁ · x₁\n",
    "w₃ = (-2, -4) + 1 · (+1) · (10, -2)\n",
    "w₃ = (-2, -4) + (10, -2)\n",
    "w₃ = (8, -6)\n",
    "\n",
    "\n",
    "Iteration 4\n",
    "Check x₂ = (12, 2), y₂ = -1:\n",
    "\n",
    "w₃ · x₂ = (8)(12) + (-6)(2) = 96 - 12 = 84\n",
    "y₂ · (w₃ · x₂) = (-1)(84) = -84 ≤ 0 ✗ Misclassified!\n",
    "\n",
    "Update:\n",
    "\n",
    "w₄ = w₃ + η · y₂ · x₂\n",
    "w₄ = (8, -6) + 1 · (-1) · (12, 2)\n",
    "w₄ = (8, -6) + (-12, -2)\n",
    "w₄ = (-4, -8)\n",
    "\n",
    "\n",
    "Iteration 5\n",
    "Check x₁ = (10, -2), y₁ = +1:\n",
    "\n",
    "w₄ · x₁ = (-4)(10) + (-8)(-2) = -40 + 16 = -24\n",
    "y₁ · (w₄ · x₁) = (+1)(-24) = -24 ≤ 0 ✗ Misclassified!\n",
    "\n",
    "Update:\n",
    "\n",
    "w₅ = w₄ + η · y₁ · x₁\n",
    "w₅ = (-4, -8) + 1 · (+1) · (10, -2)\n",
    "w₅ = (-4, -8) + (10, -2)\n",
    "w₅ = (6, -10)\n",
    "\n",
    "\n",
    "Iteration 6\n",
    "Check x₂ = (12, 2), y₂ = -1:\n",
    "\n",
    "w₅ · x₂ = (6)(12) + (-10)(2) = 72 - 20 = 52\n",
    "y₂ · (w₅ · x₂) = (-1)(52) = -52 ≤ 0 ✗ Misclassified!\n",
    "\n",
    "Update:\n",
    "\n",
    "w₆ = w₅ + η · y₂ · x₂\n",
    "w₆ = (6, -10) + 1 · (-1) · (12, 2)\n",
    "w₆ = (6, -10) + (-12, -2)\n",
    "w₆ = (-6, -12)\n",
    "\n",
    "\n",
    "Iteration 7\n",
    "Check x₁ = (10, -2), y₁ = +1:\n",
    "\n",
    "w₆ · x₁ = (-6)(10) + (-12)(-2) = -60 + 24 = -36\n",
    "y₁ · (w₆ · x₁) = (+1)(-36) = -36 ≤ 0 ✗ Misclassified!\n",
    "\n",
    "Update:\n",
    "\n",
    "w₇ = w₆ + η · y₁ · x₁\n",
    "w₇ = (-6, -12) + 1 · (+1) · (10, -2)\n",
    "w₇ = (4, -14)\n",
    "\n",
    "\n",
    "Final Check\n",
    "Check x₂ = (12, 2), y₂ = -1:\n",
    "\n",
    "w₇ · x₂ = (4)(12) + (-14)(2) = 48 - 28 = 20\n",
    "y₂ · (w₇ · x₂) = (-1)(20) = -20 ≤ 0 ✗ Still misclassified!\n",
    "\n",
    "\n",
    "Conclusion\n",
    "The pattern continues indefinitely - the Perceptron algorithm does NOT \n",
    "converge on this dataset. This is because the two points are not linearly separable\n",
    "(they cannot be separated by a straight line given their labels).\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "Update 1: x = (0, 0, 0, 0, 4), y = +1, count = 2\n",
    "\n",
    "Apply twice: w = (0, 0, 0, 0, 0) + 2(+1)(0, 0, 0, 0, 4)\n",
    "w₁ = (0, 0, 0, 0, 8)\n",
    "\n",
    "Update 2: x = (0, 0, 6, 5, 0), y = +1, count = 1\n",
    "\n",
    "w₂ = (0, 0, 0, 0, 8) + (+1)(0, 0, 6, 5, 0)\n",
    "w₂ = (0, 0, 6, 5, 8)\n",
    "\n",
    "Update 3: x = (3, 0, 0, 0, 0), y = -1, count = 1\n",
    "\n",
    "w₃ = (0, 0, 6, 5, 8) + (-1)(3, 0, 0, 0, 0)\n",
    "w₃ = (-3, 0, 6, 5, 8)\n",
    "\n",
    "Update 4: x = (0, 9, 3, 6, 0), y = -1, count = 1\n",
    "\n",
    "w₄ = (-3, 0, 6, 5, 8) + (-1)(0, 9, 3, 6, 0)\n",
    "w₄ = (-3, -9, 3, -1, 8)\n",
    "\n",
    "Update 5: x = (0, 1, 0, 2, 5), y = -1, count = 1\n",
    "\n",
    "w₅ = (-3, -9, 3, -1, 8) + (-1)(0, 1, 0, 2, 5)\n",
    "w₅ = (-3, -10, 3, -3, 3)\n",
    "\n",
    "\n",
    "Final weight vector: w = (-3, -10, 3, -3, 3)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "pythonimport numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Dataset: linearly separable points\n",
    "positive_points = np.array([[3, 4], [4, 5], [2, 3], [3, 2], [4, 3]])\n",
    "negative_points = np.array([[1, 1], [2, 1], [1, 2], [0, 1], [1, 0]])\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=1):\n",
    "        self.weights = np.array([0.0, 0.0, 0.0])  # [bias, w1, w2]\n",
    "        self.lr = learning_rate\n",
    "        self.history = []\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def predict(self, x):\n",
    "        activation = self.weights[0] + np.dot(self.weights[1:], x)\n",
    "        return 1 if activation >= 0 else -1\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        prediction = self.predict(x)\n",
    "        if prediction != y:\n",
    "            self.weights[0] += self.lr * y\n",
    "            self.weights[1:] += self.lr * y * x\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_boundary(self):\n",
    "        if self.weights[2] == 0:\n",
    "            return None, None\n",
    "        x = np.linspace(-1, 6, 100)\n",
    "        y = -(self.weights[0] + self.weights[1] * x) / self.weights[2]\n",
    "        return x, y\n",
    "\n",
    "# Initialize perceptron\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Prepare all points for training\n",
    "all_points = [(p, 1) for p in positive_points] + [(p, -1) for p in negative_points]\n",
    "\n",
    "# Store history for animation\n",
    "states = [{'weights': perceptron.weights.copy(), 'iteration': 0, 'current_point': None}]\n",
    "\n",
    "# Train and record each step\n",
    "max_epochs = 50\n",
    "for epoch in range(max_epochs):\n",
    "    converged = True\n",
    "    for point, label in all_points:\n",
    "        perceptron.iteration += 1\n",
    "        if perceptron.train_step(point, label):\n",
    "            converged = False\n",
    "            states.append({\n",
    "                'weights': perceptron.weights.copy(),\n",
    "                'iteration': perceptron.iteration,\n",
    "                'current_point': point,\n",
    "                'label': label\n",
    "            })\n",
    "    if converged:\n",
    "        print(f\"✓ Converged after {epoch + 1} epochs and {perceptron.iteration} iterations\")\n",
    "        break\n",
    "\n",
    "print(f\"Final weights: w0={perceptron.weights[0]:.2f}, w1={perceptron.weights[1]:.2f}, w2={perceptron.weights[2]:.2f}\")\n",
    "print(f\"Total updates: {len(states) - 1}\")\n",
    "print(f\"Decision boundary: {perceptron.weights[0]:.2f} + {perceptron.weights[1]:.2f}*x + {perceptron.weights[2]:.2f}*y = 0\")\n",
    "\n",
    "# Create animation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "def init():\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    return []\n",
    "\n",
    "def animate(frame):\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    \n",
    "    state = states[frame]\n",
    "    weights = state['weights']\n",
    "    current_point = state.get('current_point')\n",
    "    \n",
    "    # Left plot: Main visualization\n",
    "    ax1.set_xlim(-0.5, 5.5)\n",
    "    ax1.set_ylim(-0.5, 5.5)\n",
    "    ax1.set_xlabel('x₁', fontsize=12)\n",
    "    ax1.set_ylabel('x₂', fontsize=12)\n",
    "    ax1.set_title(f'Iteration {state[\"iteration\"]} | Weights: [{weights[0]:.2f}, {weights[1]:.2f}, {weights[2]:.2f}]', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # Plot positive points\n",
    "    for point in positive_points:\n",
    "        color = 'gold' if current_point is not None and np.array_equal(point, current_point) else 'red'\n",
    "        edgecolor = 'orange' if current_point is not None and np.array_equal(point, current_point) else 'darkred'\n",
    "        linewidth = 4 if current_point is not None and np.array_equal(point, current_point) else 2\n",
    "        ax1.scatter(point[0], point[1], c=color, s=200, edgecolors=edgecolor, \n",
    "                   linewidth=linewidth, zorder=3, label='Positive (+1)' if point[0] == positive_points[0][0] else '')\n",
    "    \n",
    "    # Plot negative points\n",
    "    for point in negative_points:\n",
    "        color = 'gold' if current_point is not None and np.array_equal(point, current_point) else 'blue'\n",
    "        edgecolor = 'orange' if current_point is not None and np.array_equal(point, current_point) else 'darkblue'\n",
    "        linewidth = 4 if current_point is not None and np.array_equal(point, current_point) else 2\n",
    "        ax1.scatter(point[0], point[1], c=color, s=200, edgecolors=edgecolor, \n",
    "                   linewidth=linewidth, zorder=3, label='Negative (-1)' if point[0] == negative_points[0][0] else '')\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    if weights[2] != 0:\n",
    "        x_line = np.linspace(-1, 6, 100)\n",
    "        y_line = -(weights[0] + weights[1] * x_line) / weights[2]\n",
    "        is_converged = frame == len(states) - 1\n",
    "        ax1.plot(x_line, y_line, \n",
    "                'g-' if is_converged else 'b--', \n",
    "                linewidth=3, \n",
    "                label='Decision Boundary (Converged)' if is_converged else 'Decision Boundary',\n",
    "                zorder=2)\n",
    "    \n",
    "    ax1.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    # Right plot: Weight evolution\n",
    "    ax2.set_xlabel('Iteration', fontsize=12)\n",
    "    ax2.set_ylabel('Weight Value', fontsize=12)\n",
    "    ax2.set_title('Weight Evolution Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    iterations = [s['iteration'] for s in states[:frame+1]]\n",
    "    w0_vals = [s['weights'][0] for s in states[:frame+1]]\n",
    "    w1_vals = [s['weights'][1] for s in states[:frame+1]]\n",
    "    w2_vals = [s['weights'][2] for s in states[:frame+1]]\n",
    "    \n",
    "    ax2.plot(iterations, w0_vals, 'ro-', label='w₀ (bias)', linewidth=2, markersize=6)\n",
    "    ax2.plot(iterations, w1_vals, 'go-', label='w₁', linewidth=2, markersize=6)\n",
    "    ax2.plot(iterations, w2_vals, 'bo-', label='w₂', linewidth=2, markersize=6)\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    \n",
    "    # Add convergence marker\n",
    "    if frame == len(states) - 1:\n",
    "        ax2.axvline(x=state['iteration'], color='green', linestyle='--', linewidth=2, alpha=0.5)\n",
    "        ax2.text(state['iteration'], ax2.get_ylim()[1] * 0.9, 'Converged', \n",
    "                rotation=90, verticalalignment='top', fontsize=10, color='green', fontweight='bold')\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Create animation\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=len(states), \n",
    "                    interval=500, blit=True, repeat=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "\n",
    "# Display animation\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6faf466-1252-4fed-90f6-db89782eaba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d17db-8e21-4a5b-ab89-cda48e0bb8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3043c86-cb1b-462c-9a34-331cf7cb2dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632ad52-b0eb-4142-8a66-e65f4b80124a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29139-c343-4892-b137-d72c6678389b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
